{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matsunori39/AI-and-Machine-Learning-for-Coders/blob/main/AI_and_Machine_Learning_for_Coders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wveHx2eBHtcI"
      },
      "source": [
        "https://www.oreilly.co.jp/books/9784873119809/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QC3Fkr4KHrJ7"
      },
      "source": [
        "# Part1: Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfSgLE9_H_C6"
      },
      "source": [
        "## Chapter 1: Overview of TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VVYlXpDIYEX"
      },
      "source": [
        "### 1.6 Getting Started in Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3T33vfGIg5W",
        "outputId": "92cdab0f-1d2c-4ace-ec19-48a92e946a01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "1/1 [==============================] - 1s 513ms/step - loss: 13.8963\n",
            "Epoch 2/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 11.1670\n",
            "Epoch 3/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 9.0149\n",
            "Epoch 4/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.3170\n",
            "Epoch 5/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5.9766\n",
            "Epoch 6/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.9175\n",
            "Epoch 7/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.0799\n",
            "Epoch 8/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.4165\n",
            "Epoch 9/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 2.8903\n",
            "Epoch 10/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.4722\n",
            "Epoch 11/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.1392\n",
            "Epoch 12/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.8732\n",
            "Epoch 13/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.6600\n",
            "Epoch 14/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.4885\n",
            "Epoch 15/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.3498\n",
            "Epoch 16/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.2370\n",
            "Epoch 17/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.1446\n",
            "Epoch 18/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.0685\n",
            "Epoch 19/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.0051\n",
            "Epoch 20/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.9518\n",
            "Epoch 21/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.9067\n",
            "Epoch 22/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.8679\n",
            "Epoch 23/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.8342\n",
            "Epoch 24/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.8045\n",
            "Epoch 25/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.7782\n",
            "Epoch 26/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.7545\n",
            "Epoch 27/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.7329\n",
            "Epoch 28/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.7130\n",
            "Epoch 29/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.6946\n",
            "Epoch 30/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.6774\n",
            "Epoch 31/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6612\n",
            "Epoch 32/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.6457\n",
            "Epoch 33/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6310\n",
            "Epoch 34/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6169\n",
            "Epoch 35/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6034\n",
            "Epoch 36/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5903\n",
            "Epoch 37/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5776\n",
            "Epoch 38/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5653\n",
            "Epoch 39/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5534\n",
            "Epoch 40/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5417\n",
            "Epoch 41/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5304\n",
            "Epoch 42/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5193\n",
            "Epoch 43/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5085\n",
            "Epoch 44/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.4980\n",
            "Epoch 45/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4877\n",
            "Epoch 46/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.4776\n",
            "Epoch 47/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.4677\n",
            "Epoch 48/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.4581\n",
            "Epoch 49/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.4486\n",
            "Epoch 50/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.4394\n",
            "Epoch 51/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4303\n",
            "Epoch 52/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.4215\n",
            "Epoch 53/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4128\n",
            "Epoch 54/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.4043\n",
            "Epoch 55/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3960\n",
            "Epoch 56/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3879\n",
            "Epoch 57/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3799\n",
            "Epoch 58/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3721\n",
            "Epoch 59/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3645\n",
            "Epoch 60/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3570\n",
            "Epoch 61/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3496\n",
            "Epoch 62/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3425\n",
            "Epoch 63/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3354\n",
            "Epoch 64/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3285\n",
            "Epoch 65/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3218\n",
            "Epoch 66/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3152\n",
            "Epoch 67/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.3087\n",
            "Epoch 68/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3024\n",
            "Epoch 69/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2961\n",
            "Epoch 70/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2901\n",
            "Epoch 71/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2841\n",
            "Epoch 72/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2783\n",
            "Epoch 73/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2725\n",
            "Epoch 74/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2670\n",
            "Epoch 75/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2615\n",
            "Epoch 76/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2561\n",
            "Epoch 77/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2508\n",
            "Epoch 78/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2457\n",
            "Epoch 79/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2406\n",
            "Epoch 80/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2357\n",
            "Epoch 81/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2309\n",
            "Epoch 82/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2261\n",
            "Epoch 83/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2215\n",
            "Epoch 84/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2169\n",
            "Epoch 85/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2125\n",
            "Epoch 86/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.2081\n",
            "Epoch 87/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.2038\n",
            "Epoch 88/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1996\n",
            "Epoch 89/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1955\n",
            "Epoch 90/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1915\n",
            "Epoch 91/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1876\n",
            "Epoch 92/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1837\n",
            "Epoch 93/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1800\n",
            "Epoch 94/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1763\n",
            "Epoch 95/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1726\n",
            "Epoch 96/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1691\n",
            "Epoch 97/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1656\n",
            "Epoch 98/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.1622\n",
            "Epoch 99/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1589\n",
            "Epoch 100/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1556\n",
            "Epoch 101/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1524\n",
            "Epoch 102/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1493\n",
            "Epoch 103/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1462\n",
            "Epoch 104/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1432\n",
            "Epoch 105/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.1403\n",
            "Epoch 106/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.1374\n",
            "Epoch 107/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.1346\n",
            "Epoch 108/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1318\n",
            "Epoch 109/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.1291\n",
            "Epoch 110/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1265\n",
            "Epoch 111/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1239\n",
            "Epoch 112/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1213\n",
            "Epoch 113/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1188\n",
            "Epoch 114/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1164\n",
            "Epoch 115/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1140\n",
            "Epoch 116/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1117\n",
            "Epoch 117/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1094\n",
            "Epoch 118/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1071\n",
            "Epoch 119/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1049\n",
            "Epoch 120/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1028\n",
            "Epoch 121/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1006\n",
            "Epoch 122/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0986\n",
            "Epoch 123/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0966\n",
            "Epoch 124/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0946\n",
            "Epoch 125/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0926\n",
            "Epoch 126/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0907\n",
            "Epoch 127/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0889\n",
            "Epoch 128/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0870\n",
            "Epoch 129/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0852\n",
            "Epoch 130/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0835\n",
            "Epoch 131/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0818\n",
            "Epoch 132/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0801\n",
            "Epoch 133/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0785\n",
            "Epoch 134/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0768\n",
            "Epoch 135/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0753\n",
            "Epoch 136/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0737\n",
            "Epoch 137/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0722\n",
            "Epoch 138/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0707\n",
            "Epoch 139/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0693\n",
            "Epoch 140/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0678\n",
            "Epoch 141/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0665\n",
            "Epoch 142/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0651\n",
            "Epoch 143/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0638\n",
            "Epoch 144/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0624\n",
            "Epoch 145/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0612\n",
            "Epoch 146/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0599\n",
            "Epoch 147/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0587\n",
            "Epoch 148/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0575\n",
            "Epoch 149/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0563\n",
            "Epoch 150/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0551\n",
            "Epoch 151/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0540\n",
            "Epoch 152/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0529\n",
            "Epoch 153/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0518\n",
            "Epoch 154/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0507\n",
            "Epoch 155/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0497\n",
            "Epoch 156/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0487\n",
            "Epoch 157/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0477\n",
            "Epoch 158/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0467\n",
            "Epoch 159/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0457\n",
            "Epoch 160/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.0448\n",
            "Epoch 161/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0439\n",
            "Epoch 162/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0430\n",
            "Epoch 163/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0421\n",
            "Epoch 164/500\n",
            "1/1 [==============================] - 0s 341ms/step - loss: 0.0412\n",
            "Epoch 165/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0404\n",
            "Epoch 166/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0396\n",
            "Epoch 167/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0387\n",
            "Epoch 168/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0379\n",
            "Epoch 169/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0372\n",
            "Epoch 170/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0364\n",
            "Epoch 171/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0357\n",
            "Epoch 172/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0349\n",
            "Epoch 173/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0342\n",
            "Epoch 174/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0335\n",
            "Epoch 175/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0328\n",
            "Epoch 176/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0321\n",
            "Epoch 177/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0315\n",
            "Epoch 178/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0308\n",
            "Epoch 179/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0302\n",
            "Epoch 180/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0296\n",
            "Epoch 181/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0290\n",
            "Epoch 182/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0284\n",
            "Epoch 183/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0278\n",
            "Epoch 184/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0272\n",
            "Epoch 185/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0267\n",
            "Epoch 186/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0261\n",
            "Epoch 187/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0256\n",
            "Epoch 188/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0251\n",
            "Epoch 189/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0245\n",
            "Epoch 190/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0240\n",
            "Epoch 191/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0235\n",
            "Epoch 192/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0231\n",
            "Epoch 193/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0226\n",
            "Epoch 194/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0221\n",
            "Epoch 195/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0217\n",
            "Epoch 196/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0212\n",
            "Epoch 197/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0208\n",
            "Epoch 198/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0204\n",
            "Epoch 199/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0199\n",
            "Epoch 200/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0195\n",
            "Epoch 201/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0191\n",
            "Epoch 202/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0187\n",
            "Epoch 203/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0184\n",
            "Epoch 204/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0180\n",
            "Epoch 205/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0176\n",
            "Epoch 206/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0172\n",
            "Epoch 207/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0169\n",
            "Epoch 208/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0165\n",
            "Epoch 209/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0162\n",
            "Epoch 210/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0159\n",
            "Epoch 211/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0155\n",
            "Epoch 212/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0152\n",
            "Epoch 213/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0149\n",
            "Epoch 214/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0146\n",
            "Epoch 215/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0143\n",
            "Epoch 216/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0140\n",
            "Epoch 217/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0137\n",
            "Epoch 218/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0134\n",
            "Epoch 219/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0132\n",
            "Epoch 220/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0129\n",
            "Epoch 221/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0126\n",
            "Epoch 222/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0124\n",
            "Epoch 223/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0121\n",
            "Epoch 224/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0119\n",
            "Epoch 225/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0116\n",
            "Epoch 226/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0114\n",
            "Epoch 227/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0112\n",
            "Epoch 228/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0109\n",
            "Epoch 229/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0107\n",
            "Epoch 230/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0105\n",
            "Epoch 231/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0103\n",
            "Epoch 232/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0101\n",
            "Epoch 233/500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0098\n",
            "Epoch 234/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0096\n",
            "Epoch 235/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0094\n",
            "Epoch 236/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0093\n",
            "Epoch 237/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0091\n",
            "Epoch 238/500\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.0089\n",
            "Epoch 239/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0087\n",
            "Epoch 240/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.0085\n",
            "Epoch 241/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0083\n",
            "Epoch 242/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.0082\n",
            "Epoch 243/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0080\n",
            "Epoch 244/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0078\n",
            "Epoch 245/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0077\n",
            "Epoch 246/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0075\n",
            "Epoch 247/500\n",
            "1/1 [==============================] - 1s 753ms/step - loss: 0.0074\n",
            "Epoch 248/500\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0072\n",
            "Epoch 249/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0071\n",
            "Epoch 250/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0069\n",
            "Epoch 251/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0068\n",
            "Epoch 252/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0066\n",
            "Epoch 253/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0065\n",
            "Epoch 254/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0064\n",
            "Epoch 255/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0062\n",
            "Epoch 256/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0061\n",
            "Epoch 257/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.0060\n",
            "Epoch 258/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.0059\n",
            "Epoch 259/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0057\n",
            "Epoch 260/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0056\n",
            "Epoch 261/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0055\n",
            "Epoch 262/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0054\n",
            "Epoch 263/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0053\n",
            "Epoch 264/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0052\n",
            "Epoch 265/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0051\n",
            "Epoch 266/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0050\n",
            "Epoch 267/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0049\n",
            "Epoch 268/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0048\n",
            "Epoch 269/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0047\n",
            "Epoch 270/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0046\n",
            "Epoch 271/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0045\n",
            "Epoch 272/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0044\n",
            "Epoch 273/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0043\n",
            "Epoch 274/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0042\n",
            "Epoch 275/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0041\n",
            "Epoch 276/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0040\n",
            "Epoch 277/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0040\n",
            "Epoch 278/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0039\n",
            "Epoch 279/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0038\n",
            "Epoch 280/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0037\n",
            "Epoch 281/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0036\n",
            "Epoch 282/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0036\n",
            "Epoch 283/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0035\n",
            "Epoch 284/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0034\n",
            "Epoch 285/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0033\n",
            "Epoch 286/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0033\n",
            "Epoch 287/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0032\n",
            "Epoch 288/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0031\n",
            "Epoch 289/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0031\n",
            "Epoch 290/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0030\n",
            "Epoch 291/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0030\n",
            "Epoch 292/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0029\n",
            "Epoch 293/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0028\n",
            "Epoch 294/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0028\n",
            "Epoch 295/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0027\n",
            "Epoch 296/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0027\n",
            "Epoch 297/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0026\n",
            "Epoch 298/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0026\n",
            "Epoch 299/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0025\n",
            "Epoch 300/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0025\n",
            "Epoch 301/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0024\n",
            "Epoch 302/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0024\n",
            "Epoch 303/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0023\n",
            "Epoch 304/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0023\n",
            "Epoch 305/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0022\n",
            "Epoch 306/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0022\n",
            "Epoch 307/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0021\n",
            "Epoch 308/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0021\n",
            "Epoch 309/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0020\n",
            "Epoch 310/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0020\n",
            "Epoch 311/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0020\n",
            "Epoch 312/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0019\n",
            "Epoch 313/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0019\n",
            "Epoch 314/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0018\n",
            "Epoch 315/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0018\n",
            "Epoch 316/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0018\n",
            "Epoch 317/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0017\n",
            "Epoch 318/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0017\n",
            "Epoch 319/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0017\n",
            "Epoch 320/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0016\n",
            "Epoch 321/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0016\n",
            "Epoch 322/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0016\n",
            "Epoch 323/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0015\n",
            "Epoch 324/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0015\n",
            "Epoch 325/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0015\n",
            "Epoch 326/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0014\n",
            "Epoch 327/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0014\n",
            "Epoch 328/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0014\n",
            "Epoch 329/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0013\n",
            "Epoch 330/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0013\n",
            "Epoch 331/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0013\n",
            "Epoch 332/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0013\n",
            "Epoch 333/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 334/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 335/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0012\n",
            "Epoch 336/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0012\n",
            "Epoch 337/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0011\n",
            "Epoch 338/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0011\n",
            "Epoch 339/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0011\n",
            "Epoch 340/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0011\n",
            "Epoch 341/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0010\n",
            "Epoch 342/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0010\n",
            "Epoch 343/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0010\n",
            "Epoch 344/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 9.8350e-04\n",
            "Epoch 345/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 9.6329e-04\n",
            "Epoch 346/500\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 9.4351e-04\n",
            "Epoch 347/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 9.2413e-04\n",
            "Epoch 348/500\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 9.0514e-04\n",
            "Epoch 349/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8.8655e-04\n",
            "Epoch 350/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 8.6834e-04\n",
            "Epoch 351/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 8.5051e-04\n",
            "Epoch 352/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 8.3304e-04\n",
            "Epoch 353/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 8.1592e-04\n",
            "Epoch 354/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 7.9916e-04\n",
            "Epoch 355/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 7.8275e-04\n",
            "Epoch 356/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.6667e-04\n",
            "Epoch 357/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 7.5093e-04\n",
            "Epoch 358/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 7.3550e-04\n",
            "Epoch 359/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 7.2039e-04\n",
            "Epoch 360/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 7.0560e-04\n",
            "Epoch 361/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.9110e-04\n",
            "Epoch 362/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.7691e-04\n",
            "Epoch 363/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.6300e-04\n",
            "Epoch 364/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.4938e-04\n",
            "Epoch 365/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.3604e-04\n",
            "Epoch 366/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.2298e-04\n",
            "Epoch 367/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.1018e-04\n",
            "Epoch 368/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 5.9765e-04\n",
            "Epoch 369/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 5.8537e-04\n",
            "Epoch 370/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5.7335e-04\n",
            "Epoch 371/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 5.6157e-04\n",
            "Epoch 372/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 5.5004e-04\n",
            "Epoch 373/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 5.3874e-04\n",
            "Epoch 374/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 5.2767e-04\n",
            "Epoch 375/500\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 5.1684e-04\n",
            "Epoch 376/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 5.0622e-04\n",
            "Epoch 377/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.9582e-04\n",
            "Epoch 378/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.8563e-04\n",
            "Epoch 379/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.7566e-04\n",
            "Epoch 380/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 4.6589e-04\n",
            "Epoch 381/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.5632e-04\n",
            "Epoch 382/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.4695e-04\n",
            "Epoch 383/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.3777e-04\n",
            "Epoch 384/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 4.2878e-04\n",
            "Epoch 385/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 4.1997e-04\n",
            "Epoch 386/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 4.1134e-04\n",
            "Epoch 387/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 4.0289e-04\n",
            "Epoch 388/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.9462e-04\n",
            "Epoch 389/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.8651e-04\n",
            "Epoch 390/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.7857e-04\n",
            "Epoch 391/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.7080e-04\n",
            "Epoch 392/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.6318e-04\n",
            "Epoch 393/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.5572e-04\n",
            "Epoch 394/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 3.4842e-04\n",
            "Epoch 395/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 3.4126e-04\n",
            "Epoch 396/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.3425e-04\n",
            "Epoch 397/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 3.2738e-04\n",
            "Epoch 398/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 3.2066e-04\n",
            "Epoch 399/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 3.1407e-04\n",
            "Epoch 400/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.0762e-04\n",
            "Epoch 401/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 3.0130e-04\n",
            "Epoch 402/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.9511e-04\n",
            "Epoch 403/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 2.8905e-04\n",
            "Epoch 404/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.8311e-04\n",
            "Epoch 405/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 2.7730e-04\n",
            "Epoch 406/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.7160e-04\n",
            "Epoch 407/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.6602e-04\n",
            "Epoch 408/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.6056e-04\n",
            "Epoch 409/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 2.5521e-04\n",
            "Epoch 410/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.4996e-04\n",
            "Epoch 411/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.4483e-04\n",
            "Epoch 412/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.3980e-04\n",
            "Epoch 413/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 2.3488e-04\n",
            "Epoch 414/500\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 2.3005e-04\n",
            "Epoch 415/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.2533e-04\n",
            "Epoch 416/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.2070e-04\n",
            "Epoch 417/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 2.1617e-04\n",
            "Epoch 418/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.1173e-04\n",
            "Epoch 419/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.0738e-04\n",
            "Epoch 420/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.0312e-04\n",
            "Epoch 421/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.9895e-04\n",
            "Epoch 422/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.9486e-04\n",
            "Epoch 423/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.9086e-04\n",
            "Epoch 424/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.8694e-04\n",
            "Epoch 425/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.8310e-04\n",
            "Epoch 426/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.7934e-04\n",
            "Epoch 427/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.7565e-04\n",
            "Epoch 428/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.7204e-04\n",
            "Epoch 429/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.6851e-04\n",
            "Epoch 430/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.6505e-04\n",
            "Epoch 431/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.6166e-04\n",
            "Epoch 432/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.5834e-04\n",
            "Epoch 433/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.5508e-04\n",
            "Epoch 434/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.5190e-04\n",
            "Epoch 435/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 1.4878e-04\n",
            "Epoch 436/500\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.4572e-04\n",
            "Epoch 437/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.4273e-04\n",
            "Epoch 438/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.3980e-04\n",
            "Epoch 439/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.3692e-04\n",
            "Epoch 440/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.3411e-04\n",
            "Epoch 441/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.3136e-04\n",
            "Epoch 442/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.2866e-04\n",
            "Epoch 443/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.2602e-04\n",
            "Epoch 444/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.2343e-04\n",
            "Epoch 445/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.2089e-04\n",
            "Epoch 446/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.1841e-04\n",
            "Epoch 447/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.1598e-04\n",
            "Epoch 448/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1360e-04\n",
            "Epoch 449/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.1126e-04\n",
            "Epoch 450/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.0898e-04\n",
            "Epoch 451/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.0674e-04\n",
            "Epoch 452/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.0455e-04\n",
            "Epoch 453/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0240e-04\n",
            "Epoch 454/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.0029e-04\n",
            "Epoch 455/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 9.8234e-05\n",
            "Epoch 456/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 9.6216e-05\n",
            "Epoch 457/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 9.4240e-05\n",
            "Epoch 458/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 9.2305e-05\n",
            "Epoch 459/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 9.0409e-05\n",
            "Epoch 460/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 8.8552e-05\n",
            "Epoch 461/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8.6733e-05\n",
            "Epoch 462/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 8.4951e-05\n",
            "Epoch 463/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 8.3206e-05\n",
            "Epoch 464/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 8.1497e-05\n",
            "Epoch 465/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.9823e-05\n",
            "Epoch 466/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 7.8183e-05\n",
            "Epoch 467/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 7.6576e-05\n",
            "Epoch 468/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 7.5003e-05\n",
            "Epoch 469/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 7.3464e-05\n",
            "Epoch 470/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 7.1954e-05\n",
            "Epoch 471/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 7.0476e-05\n",
            "Epoch 472/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 6.9029e-05\n",
            "Epoch 473/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.7611e-05\n",
            "Epoch 474/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 6.6222e-05\n",
            "Epoch 475/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 6.4862e-05\n",
            "Epoch 476/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.3530e-05\n",
            "Epoch 477/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 6.2224e-05\n",
            "Epoch 478/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.0947e-05\n",
            "Epoch 479/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 5.9693e-05\n",
            "Epoch 480/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 5.8468e-05\n",
            "Epoch 481/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 5.7267e-05\n",
            "Epoch 482/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5.6090e-05\n",
            "Epoch 483/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 5.4939e-05\n",
            "Epoch 484/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 5.3811e-05\n",
            "Epoch 485/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5.2704e-05\n",
            "Epoch 486/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 5.1623e-05\n",
            "Epoch 487/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 5.0561e-05\n",
            "Epoch 488/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 4.9523e-05\n",
            "Epoch 489/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 4.8507e-05\n",
            "Epoch 490/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 4.7510e-05\n",
            "Epoch 491/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.6534e-05\n",
            "Epoch 492/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 4.5579e-05\n",
            "Epoch 493/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.4643e-05\n",
            "Epoch 494/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 4.3725e-05\n",
            "Epoch 495/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.2827e-05\n",
            "Epoch 496/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 4.1948e-05\n",
            "Epoch 497/500\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 4.1086e-05\n",
            "Epoch 498/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.0242e-05\n",
            "Epoch 499/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.9415e-05\n",
            "Epoch 500/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.8606e-05\n",
            "[[18.981873]]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential([Dense(units=1, input_shape=[1])])\n",
        "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
        "\n",
        "xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
        "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)\n",
        "\n",
        "model.fit(xs, ys, epochs=500)\n",
        "\n",
        "print(model.predict([10.0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YENPfdOJUC2",
        "outputId": "924fe7e1-c6d7-4ec9-e74e-5d1b5038cf3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "1/1 [==============================] - 1s 656ms/step - loss: 0.5646\n",
            "Epoch 2/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5521\n",
            "Epoch 3/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5401\n",
            "Epoch 4/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5285\n",
            "Epoch 5/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5172\n",
            "Epoch 6/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5063\n",
            "Epoch 7/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4956\n",
            "Epoch 8/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.4853\n",
            "Epoch 9/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4751\n",
            "Epoch 10/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4652\n",
            "Epoch 11/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.4556\n",
            "Epoch 12/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4462\n",
            "Epoch 13/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4369\n",
            "Epoch 14/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.4279\n",
            "Epoch 15/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.4191\n",
            "Epoch 16/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.4104\n",
            "Epoch 17/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.4020\n",
            "Epoch 18/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3937\n",
            "Epoch 19/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.3856\n",
            "Epoch 20/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.3777\n",
            "Epoch 21/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.3699\n",
            "Epoch 22/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.3623\n",
            "Epoch 23/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3549\n",
            "Epoch 24/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3476\n",
            "Epoch 25/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3404\n",
            "Epoch 26/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3334\n",
            "Epoch 27/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3266\n",
            "Epoch 28/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3199\n",
            "Epoch 29/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3133\n",
            "Epoch 30/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3069\n",
            "Epoch 31/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3006\n",
            "Epoch 32/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.2944\n",
            "Epoch 33/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.2883\n",
            "Epoch 34/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.2824\n",
            "Epoch 35/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.2766\n",
            "Epoch 36/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.2709\n",
            "Epoch 37/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.2654\n",
            "Epoch 38/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.2599\n",
            "Epoch 39/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.2546\n",
            "Epoch 40/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2493\n",
            "Epoch 41/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2442\n",
            "Epoch 42/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2392\n",
            "Epoch 43/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2343\n",
            "Epoch 44/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2295\n",
            "Epoch 45/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2248\n",
            "Epoch 46/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2202\n",
            "Epoch 47/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2156\n",
            "Epoch 48/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2112\n",
            "Epoch 49/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.2069\n",
            "Epoch 50/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2026\n",
            "Epoch 51/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1985\n",
            "Epoch 52/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1944\n",
            "Epoch 53/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.1904\n",
            "Epoch 54/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1865\n",
            "Epoch 55/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1826\n",
            "Epoch 56/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1789\n",
            "Epoch 57/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.1752\n",
            "Epoch 58/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1716\n",
            "Epoch 59/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.1681\n",
            "Epoch 60/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1646\n",
            "Epoch 61/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1613\n",
            "Epoch 62/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.1579\n",
            "Epoch 63/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.1547\n",
            "Epoch 64/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1515\n",
            "Epoch 65/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.1484\n",
            "Epoch 66/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1454\n",
            "Epoch 67/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.1424\n",
            "Epoch 68/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1395\n",
            "Epoch 69/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.1366\n",
            "Epoch 70/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1338\n",
            "Epoch 71/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.1310\n",
            "Epoch 72/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.1283\n",
            "Epoch 73/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1257\n",
            "Epoch 74/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.1231\n",
            "Epoch 75/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1206\n",
            "Epoch 76/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1181\n",
            "Epoch 77/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1157\n",
            "Epoch 78/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.1133\n",
            "Epoch 79/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1110\n",
            "Epoch 80/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1087\n",
            "Epoch 81/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1065\n",
            "Epoch 82/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.1043\n",
            "Epoch 83/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.1021\n",
            "Epoch 84/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.1000\n",
            "Epoch 85/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0980\n",
            "Epoch 86/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0960\n",
            "Epoch 87/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0940\n",
            "Epoch 88/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.0921\n",
            "Epoch 89/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0902\n",
            "Epoch 90/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0883\n",
            "Epoch 91/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.0865\n",
            "Epoch 92/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0847\n",
            "Epoch 93/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0830\n",
            "Epoch 94/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0813\n",
            "Epoch 95/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0796\n",
            "Epoch 96/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.0780\n",
            "Epoch 97/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.0764\n",
            "Epoch 98/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0748\n",
            "Epoch 99/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0733\n",
            "Epoch 100/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0718\n",
            "Epoch 101/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0703\n",
            "Epoch 102/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0689\n",
            "Epoch 103/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0674\n",
            "Epoch 104/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.0661\n",
            "Epoch 105/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0647\n",
            "Epoch 106/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0634\n",
            "Epoch 107/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0621\n",
            "Epoch 108/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0608\n",
            "Epoch 109/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0595\n",
            "Epoch 110/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0583\n",
            "Epoch 111/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0571\n",
            "Epoch 112/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0560\n",
            "Epoch 113/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0548\n",
            "Epoch 114/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0537\n",
            "Epoch 115/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0526\n",
            "Epoch 116/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0515\n",
            "Epoch 117/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0504\n",
            "Epoch 118/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0494\n",
            "Epoch 119/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0484\n",
            "Epoch 120/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0474\n",
            "Epoch 121/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0464\n",
            "Epoch 122/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0455\n",
            "Epoch 123/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0445\n",
            "Epoch 124/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0436\n",
            "Epoch 125/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0427\n",
            "Epoch 126/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0418\n",
            "Epoch 127/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0410\n",
            "Epoch 128/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0401\n",
            "Epoch 129/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0393\n",
            "Epoch 130/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0385\n",
            "Epoch 131/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0377\n",
            "Epoch 132/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0369\n",
            "Epoch 133/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0362\n",
            "Epoch 134/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0354\n",
            "Epoch 135/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0347\n",
            "Epoch 136/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0340\n",
            "Epoch 137/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0333\n",
            "Epoch 138/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0326\n",
            "Epoch 139/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0319\n",
            "Epoch 140/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.0313\n",
            "Epoch 141/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0307\n",
            "Epoch 142/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0300\n",
            "Epoch 143/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0294\n",
            "Epoch 144/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0288\n",
            "Epoch 145/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0282\n",
            "Epoch 146/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0276\n",
            "Epoch 147/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0271\n",
            "Epoch 148/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0265\n",
            "Epoch 149/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0260\n",
            "Epoch 150/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0254\n",
            "Epoch 151/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0249\n",
            "Epoch 152/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0244\n",
            "Epoch 153/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0239\n",
            "Epoch 154/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0234\n",
            "Epoch 155/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0229\n",
            "Epoch 156/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0225\n",
            "Epoch 157/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0220\n",
            "Epoch 158/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0215\n",
            "Epoch 159/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0211\n",
            "Epoch 160/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0207\n",
            "Epoch 161/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0202\n",
            "Epoch 162/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0198\n",
            "Epoch 163/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0194\n",
            "Epoch 164/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0190\n",
            "Epoch 165/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0186\n",
            "Epoch 166/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0182\n",
            "Epoch 167/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0179\n",
            "Epoch 168/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.0175\n",
            "Epoch 169/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0171\n",
            "Epoch 170/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.0168\n",
            "Epoch 171/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0164\n",
            "Epoch 172/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0161\n",
            "Epoch 173/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0158\n",
            "Epoch 174/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0155\n",
            "Epoch 175/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.0151\n",
            "Epoch 176/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0148\n",
            "Epoch 177/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0145\n",
            "Epoch 178/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0142\n",
            "Epoch 179/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0139\n",
            "Epoch 180/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0136\n",
            "Epoch 181/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0134\n",
            "Epoch 182/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0131\n",
            "Epoch 183/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0128\n",
            "Epoch 184/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0126\n",
            "Epoch 185/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0123\n",
            "Epoch 186/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0120\n",
            "Epoch 187/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0118\n",
            "Epoch 188/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0116\n",
            "Epoch 189/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0113\n",
            "Epoch 190/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0111\n",
            "Epoch 191/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0109\n",
            "Epoch 192/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0106\n",
            "Epoch 193/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0104\n",
            "Epoch 194/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0102\n",
            "Epoch 195/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0100\n",
            "Epoch 196/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0098\n",
            "Epoch 197/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0096\n",
            "Epoch 198/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0094\n",
            "Epoch 199/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0092\n",
            "Epoch 200/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0090\n",
            "Epoch 201/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0088\n",
            "Epoch 202/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0086\n",
            "Epoch 203/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0085\n",
            "Epoch 204/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0083\n",
            "Epoch 205/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0081\n",
            "Epoch 206/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0080\n",
            "Epoch 207/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0078\n",
            "Epoch 208/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0076\n",
            "Epoch 209/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0075\n",
            "Epoch 210/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0073\n",
            "Epoch 211/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0072\n",
            "Epoch 212/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0070\n",
            "Epoch 213/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.0069\n",
            "Epoch 214/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0067\n",
            "Epoch 215/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0066\n",
            "Epoch 216/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0065\n",
            "Epoch 217/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0063\n",
            "Epoch 218/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0062\n",
            "Epoch 219/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0061\n",
            "Epoch 220/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0059\n",
            "Epoch 221/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0058\n",
            "Epoch 222/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0057\n",
            "Epoch 223/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.0056\n",
            "Epoch 224/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0055\n",
            "Epoch 225/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0054\n",
            "Epoch 226/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0053\n",
            "Epoch 227/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0051\n",
            "Epoch 228/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.0050\n",
            "Epoch 229/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0049\n",
            "Epoch 230/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0048\n",
            "Epoch 231/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0047\n",
            "Epoch 232/500\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0046\n",
            "Epoch 233/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0045\n",
            "Epoch 234/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0044\n",
            "Epoch 235/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.0044\n",
            "Epoch 236/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.0043\n",
            "Epoch 237/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0042\n",
            "Epoch 238/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0041\n",
            "Epoch 239/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0040\n",
            "Epoch 240/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.0039\n",
            "Epoch 241/500\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0038\n",
            "Epoch 242/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0038\n",
            "Epoch 243/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0037\n",
            "Epoch 244/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.0036\n",
            "Epoch 245/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0035\n",
            "Epoch 246/500\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.0035\n",
            "Epoch 247/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0034\n",
            "Epoch 248/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0033\n",
            "Epoch 249/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0033\n",
            "Epoch 250/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0032\n",
            "Epoch 251/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0031\n",
            "Epoch 252/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0031\n",
            "Epoch 253/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0030\n",
            "Epoch 254/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0029\n",
            "Epoch 255/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0029\n",
            "Epoch 256/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0028\n",
            "Epoch 257/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0028\n",
            "Epoch 258/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0027\n",
            "Epoch 259/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0026\n",
            "Epoch 260/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0026\n",
            "Epoch 261/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0025\n",
            "Epoch 262/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0025\n",
            "Epoch 263/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0024\n",
            "Epoch 264/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0024\n",
            "Epoch 265/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0023\n",
            "Epoch 266/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0023\n",
            "Epoch 267/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0022\n",
            "Epoch 268/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0022\n",
            "Epoch 269/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0022\n",
            "Epoch 270/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0021\n",
            "Epoch 271/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0021\n",
            "Epoch 272/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0020\n",
            "Epoch 273/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0020\n",
            "Epoch 274/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0019\n",
            "Epoch 275/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0019\n",
            "Epoch 276/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0019\n",
            "Epoch 277/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0018\n",
            "Epoch 278/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0018\n",
            "Epoch 279/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0017\n",
            "Epoch 280/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0017\n",
            "Epoch 281/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0017\n",
            "Epoch 282/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0016\n",
            "Epoch 283/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0016\n",
            "Epoch 284/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0016\n",
            "Epoch 285/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0015\n",
            "Epoch 286/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0015\n",
            "Epoch 287/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0015\n",
            "Epoch 288/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0015\n",
            "Epoch 289/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0014\n",
            "Epoch 290/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0014\n",
            "Epoch 291/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0014\n",
            "Epoch 292/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 293/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 294/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 295/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 296/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 297/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 298/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 299/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0012\n",
            "Epoch 300/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 301/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 302/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 303/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0011\n",
            "Epoch 304/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 305/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 306/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 9.9813e-04\n",
            "Epoch 307/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 9.7763e-04\n",
            "Epoch 308/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 9.5755e-04\n",
            "Epoch 309/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 9.3788e-04\n",
            "Epoch 310/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 9.1862e-04\n",
            "Epoch 311/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 8.9975e-04\n",
            "Epoch 312/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 8.8126e-04\n",
            "Epoch 313/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 8.6316e-04\n",
            "Epoch 314/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 8.4543e-04\n",
            "Epoch 315/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 8.2807e-04\n",
            "Epoch 316/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 8.1106e-04\n",
            "Epoch 317/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 7.9440e-04\n",
            "Epoch 318/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 7.7808e-04\n",
            "Epoch 319/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 7.6210e-04\n",
            "Epoch 320/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 7.4644e-04\n",
            "Epoch 321/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 7.3111e-04\n",
            "Epoch 322/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 7.1609e-04\n",
            "Epoch 323/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.0138e-04\n",
            "Epoch 324/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.8698e-04\n",
            "Epoch 325/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.7286e-04\n",
            "Epoch 326/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.5905e-04\n",
            "Epoch 327/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.4551e-04\n",
            "Epoch 328/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.3225e-04\n",
            "Epoch 329/500\n",
            "1/1 [==============================] - 0s 276ms/step - loss: 6.1926e-04\n",
            "Epoch 330/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.0654e-04\n",
            "Epoch 331/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 5.9409e-04\n",
            "Epoch 332/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 5.8188e-04\n",
            "Epoch 333/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 5.6993e-04\n",
            "Epoch 334/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 5.5822e-04\n",
            "Epoch 335/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 5.4676e-04\n",
            "Epoch 336/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 5.3553e-04\n",
            "Epoch 337/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 5.2453e-04\n",
            "Epoch 338/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 5.1375e-04\n",
            "Epoch 339/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 5.0320e-04\n",
            "Epoch 340/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.9286e-04\n",
            "Epoch 341/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.8274e-04\n",
            "Epoch 342/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.7282e-04\n",
            "Epoch 343/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.6311e-04\n",
            "Epoch 344/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.5360e-04\n",
            "Epoch 345/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.4428e-04\n",
            "Epoch 346/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.3516e-04\n",
            "Epoch 347/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.2622e-04\n",
            "Epoch 348/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.1746e-04\n",
            "Epoch 349/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.0889e-04\n",
            "Epoch 350/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.0049e-04\n",
            "Epoch 351/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.9227e-04\n",
            "Epoch 352/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.8421e-04\n",
            "Epoch 353/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.7631e-04\n",
            "Epoch 354/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.6858e-04\n",
            "Epoch 355/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.6101e-04\n",
            "Epoch 356/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.5360e-04\n",
            "Epoch 357/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.4634e-04\n",
            "Epoch 358/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.3922e-04\n",
            "Epoch 359/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.3225e-04\n",
            "Epoch 360/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.2543e-04\n",
            "Epoch 361/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.1875e-04\n",
            "Epoch 362/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.1220e-04\n",
            "Epoch 363/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.0578e-04\n",
            "Epoch 364/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 2.9950e-04\n",
            "Epoch 365/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 2.9335e-04\n",
            "Epoch 366/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 2.8732e-04\n",
            "Epoch 367/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 2.8142e-04\n",
            "Epoch 368/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 2.7564e-04\n",
            "Epoch 369/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 2.6998e-04\n",
            "Epoch 370/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 2.6444e-04\n",
            "Epoch 371/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 2.5900e-04\n",
            "Epoch 372/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 2.5368e-04\n",
            "Epoch 373/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 2.4847e-04\n",
            "Epoch 374/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 2.4337e-04\n",
            "Epoch 375/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 2.3837e-04\n",
            "Epoch 376/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 2.3347e-04\n",
            "Epoch 377/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 2.2868e-04\n",
            "Epoch 378/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 2.2398e-04\n",
            "Epoch 379/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 2.1938e-04\n",
            "Epoch 380/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 2.1488e-04\n",
            "Epoch 381/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 2.1046e-04\n",
            "Epoch 382/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 2.0614e-04\n",
            "Epoch 383/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 2.0190e-04\n",
            "Epoch 384/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.9776e-04\n",
            "Epoch 385/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.9370e-04\n",
            "Epoch 386/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.8972e-04\n",
            "Epoch 387/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.8582e-04\n",
            "Epoch 388/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.8200e-04\n",
            "Epoch 389/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.7827e-04\n",
            "Epoch 390/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.7460e-04\n",
            "Epoch 391/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.7102e-04\n",
            "Epoch 392/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.6751e-04\n",
            "Epoch 393/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.6407e-04\n",
            "Epoch 394/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.6069e-04\n",
            "Epoch 395/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.5740e-04\n",
            "Epoch 396/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.5416e-04\n",
            "Epoch 397/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.5099e-04\n",
            "Epoch 398/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.4789e-04\n",
            "Epoch 399/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.4486e-04\n",
            "Epoch 400/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.4188e-04\n",
            "Epoch 401/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.3897e-04\n",
            "Epoch 402/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.3611e-04\n",
            "Epoch 403/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.3332e-04\n",
            "Epoch 404/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.3058e-04\n",
            "Epoch 405/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.2790e-04\n",
            "Epoch 406/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.2527e-04\n",
            "Epoch 407/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.2269e-04\n",
            "Epoch 408/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.2018e-04\n",
            "Epoch 409/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.1771e-04\n",
            "Epoch 410/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.1529e-04\n",
            "Epoch 411/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.1292e-04\n",
            "Epoch 412/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.1060e-04\n",
            "Epoch 413/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.0833e-04\n",
            "Epoch 414/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.0611e-04\n",
            "Epoch 415/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.0393e-04\n",
            "Epoch 416/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.0179e-04\n",
            "Epoch 417/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 9.9700e-05\n",
            "Epoch 418/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 9.7652e-05\n",
            "Epoch 419/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 9.5647e-05\n",
            "Epoch 420/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 9.3683e-05\n",
            "Epoch 421/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 9.1757e-05\n",
            "Epoch 422/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 8.9873e-05\n",
            "Epoch 423/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 8.8026e-05\n",
            "Epoch 424/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8.6217e-05\n",
            "Epoch 425/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 8.4447e-05\n",
            "Epoch 426/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 8.2713e-05\n",
            "Epoch 427/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 8.1014e-05\n",
            "Epoch 428/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.9349e-05\n",
            "Epoch 429/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.7719e-05\n",
            "Epoch 430/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.6122e-05\n",
            "Epoch 431/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.4558e-05\n",
            "Epoch 432/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.3027e-05\n",
            "Epoch 433/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.1527e-05\n",
            "Epoch 434/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.0059e-05\n",
            "Epoch 435/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.8620e-05\n",
            "Epoch 436/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.7209e-05\n",
            "Epoch 437/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.5828e-05\n",
            "Epoch 438/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.4477e-05\n",
            "Epoch 439/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.3153e-05\n",
            "Epoch 440/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.1855e-05\n",
            "Epoch 441/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.0586e-05\n",
            "Epoch 442/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 5.9341e-05\n",
            "Epoch 443/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 5.8122e-05\n",
            "Epoch 444/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 5.6927e-05\n",
            "Epoch 445/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.5759e-05\n",
            "Epoch 446/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 5.4614e-05\n",
            "Epoch 447/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 5.3493e-05\n",
            "Epoch 448/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.2393e-05\n",
            "Epoch 449/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5.1316e-05\n",
            "Epoch 450/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 5.0263e-05\n",
            "Epoch 451/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.9230e-05\n",
            "Epoch 452/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.8219e-05\n",
            "Epoch 453/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.7229e-05\n",
            "Epoch 454/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 4.6258e-05\n",
            "Epoch 455/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.5308e-05\n",
            "Epoch 456/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.4378e-05\n",
            "Epoch 457/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.3466e-05\n",
            "Epoch 458/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.2573e-05\n",
            "Epoch 459/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.1698e-05\n",
            "Epoch 460/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.0841e-05\n",
            "Epoch 461/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.0003e-05\n",
            "Epoch 462/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.9181e-05\n",
            "Epoch 463/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.8376e-05\n",
            "Epoch 464/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.7589e-05\n",
            "Epoch 465/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 3.6816e-05\n",
            "Epoch 466/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.6060e-05\n",
            "Epoch 467/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.5319e-05\n",
            "Epoch 468/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.4593e-05\n",
            "Epoch 469/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.3883e-05\n",
            "Epoch 470/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.3187e-05\n",
            "Epoch 471/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 3.2505e-05\n",
            "Epoch 472/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.1838e-05\n",
            "Epoch 473/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.1184e-05\n",
            "Epoch 474/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.0543e-05\n",
            "Epoch 475/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.9916e-05\n",
            "Epoch 476/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.9301e-05\n",
            "Epoch 477/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.8700e-05\n",
            "Epoch 478/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.8110e-05\n",
            "Epoch 479/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.7533e-05\n",
            "Epoch 480/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.6968e-05\n",
            "Epoch 481/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.6414e-05\n",
            "Epoch 482/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.5871e-05\n",
            "Epoch 483/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.5340e-05\n",
            "Epoch 484/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.4819e-05\n",
            "Epoch 485/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.4309e-05\n",
            "Epoch 486/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.3810e-05\n",
            "Epoch 487/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.3321e-05\n",
            "Epoch 488/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.2842e-05\n",
            "Epoch 489/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.2372e-05\n",
            "Epoch 490/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.1913e-05\n",
            "Epoch 491/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.1463e-05\n",
            "Epoch 492/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.1023e-05\n",
            "Epoch 493/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.0591e-05\n",
            "Epoch 494/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.0167e-05\n",
            "Epoch 495/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.9754e-05\n",
            "Epoch 496/500\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 1.9348e-05\n",
            "Epoch 497/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.8950e-05\n",
            "Epoch 498/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.8561e-05\n",
            "Epoch 499/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.8180e-05\n",
            "Epoch 500/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.7806e-05\n",
            "[[18.987688]]\n",
            "Here is what I learned: [array([[1.9982157]], dtype=float32), array([-0.9944679], dtype=float32)]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "l0 = Dense(units=1, input_shape=[1])\n",
        "model = Sequential([l0])\n",
        "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
        "\n",
        "xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
        "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)\n",
        "\n",
        "model.fit(xs, ys, epochs=500)\n",
        "\n",
        "print(model.predict([10.0]))\n",
        "print(\"Here is what I learned: {}\".format(l0.get_weights()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnjWyl-kNCxI"
      },
      "source": [
        "## Chapter 2: Introduction to Computer Vision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eAjhGmgWZt6"
      },
      "source": [
        "### 2.3 Design of Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB8StKQEWl-v"
      },
      "source": [
        "#### Entire Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTZOdpg1Wrze",
        "outputId": "35214205-bf5a-4a6e-effc-ca7f412aef17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.4982 - accuracy: 0.8240\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3769 - accuracy: 0.8656\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3407 - accuracy: 0.8755\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3145 - accuracy: 0.8841\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2956 - accuracy: 0.8925\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3144d34d10>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "data = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(training_images, training_labels), (test_images, test_labels) = data.load_data()\n",
        "\n",
        "training_images = training_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "            tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "            tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "        ])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoNq0nrGgBGK",
        "outputId": "5c171939-1682-4614-897c-2d1cd5533794"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 1ms/step - loss: 0.3438 - accuracy: 0.8780\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.34376367926597595, 0.878000020980835]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "model.evaluate(test_images, test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWPZVlCwgnJI"
      },
      "source": [
        "### 2.5 Investigation of model outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWcpXEY7g_jL",
        "outputId": "b18092a8-f242-4351-8c85-7a380bec3249"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5.0000585e-06 1.5528199e-09 9.3468984e-08 3.1159952e-09 1.1803818e-07\n",
            " 2.5308020e-02 8.8974775e-07 9.6376903e-02 3.1608484e-05 8.7827736e-01]\n",
            "9\n"
          ]
        }
      ],
      "source": [
        "classifications = model.predict(test_images)\n",
        "print(classifications[0])\n",
        "print(test_labels[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hN4TA9DAVRL5"
      },
      "source": [
        "### 2.6 Increased number of studies - Finding overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XljvVUFWC8r",
        "outputId": "0c99bb0f-18a3-49a5-918b-104d4e258f77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.5013 - accuracy: 0.8256\n",
            "Epoch 2/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3762 - accuracy: 0.8640\n",
            "Epoch 3/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3375 - accuracy: 0.8772\n",
            "Epoch 4/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3138 - accuracy: 0.8848\n",
            "Epoch 5/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2950 - accuracy: 0.8918\n",
            "Epoch 6/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2821 - accuracy: 0.8949\n",
            "Epoch 7/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2688 - accuracy: 0.9008\n",
            "Epoch 8/50\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2584 - accuracy: 0.9046\n",
            "Epoch 9/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2482 - accuracy: 0.9076\n",
            "Epoch 10/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2368 - accuracy: 0.9115\n",
            "Epoch 11/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2302 - accuracy: 0.9141\n",
            "Epoch 12/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2239 - accuracy: 0.9162\n",
            "Epoch 13/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2178 - accuracy: 0.9179\n",
            "Epoch 14/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2087 - accuracy: 0.9219\n",
            "Epoch 15/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2036 - accuracy: 0.9237\n",
            "Epoch 16/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1950 - accuracy: 0.9278\n",
            "Epoch 17/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1914 - accuracy: 0.9286\n",
            "Epoch 18/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1874 - accuracy: 0.9291\n",
            "Epoch 19/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1832 - accuracy: 0.9317\n",
            "Epoch 20/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1770 - accuracy: 0.9338\n",
            "Epoch 21/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1729 - accuracy: 0.9347\n",
            "Epoch 22/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1684 - accuracy: 0.9370\n",
            "Epoch 23/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1645 - accuracy: 0.9376\n",
            "Epoch 24/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1585 - accuracy: 0.9405\n",
            "Epoch 25/50\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1576 - accuracy: 0.9412\n",
            "Epoch 26/50\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1528 - accuracy: 0.9423\n",
            "Epoch 27/50\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1502 - accuracy: 0.9433\n",
            "Epoch 28/50\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1454 - accuracy: 0.9454\n",
            "Epoch 29/50\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1446 - accuracy: 0.9452\n",
            "Epoch 30/50\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1407 - accuracy: 0.9457\n",
            "Epoch 31/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1365 - accuracy: 0.9486\n",
            "Epoch 32/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1357 - accuracy: 0.9494\n",
            "Epoch 33/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1323 - accuracy: 0.9500\n",
            "Epoch 34/50\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.1306 - accuracy: 0.9507\n",
            "Epoch 35/50\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1271 - accuracy: 0.9524\n",
            "Epoch 36/50\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1233 - accuracy: 0.9540\n",
            "Epoch 37/50\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1208 - accuracy: 0.9551\n",
            "Epoch 38/50\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1191 - accuracy: 0.9552\n",
            "Epoch 39/50\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.1176 - accuracy: 0.9558\n",
            "Epoch 40/50\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.1137 - accuracy: 0.9569\n",
            "Epoch 41/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1133 - accuracy: 0.9576\n",
            "Epoch 42/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1107 - accuracy: 0.9584\n",
            "Epoch 43/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1102 - accuracy: 0.9592\n",
            "Epoch 44/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1083 - accuracy: 0.9594\n",
            "Epoch 45/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1047 - accuracy: 0.9613\n",
            "Epoch 46/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1035 - accuracy: 0.9609\n",
            "Epoch 47/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1022 - accuracy: 0.9612\n",
            "Epoch 48/50\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0973 - accuracy: 0.9631\n",
            "Epoch 49/50\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0986 - accuracy: 0.9621\n",
            "Epoch 50/50\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0984 - accuracy: 0.9628\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f314002b2d0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "            tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "            tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "        ])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuS0jXO4XSxI",
        "outputId": "83eabaa7-22e8-4c23-d6c9-25d02db4142d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.5213 - accuracy: 0.8871\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5212934017181396, 0.8870999813079834]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "model.evaluate(test_images, test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r22zCDMXW7H"
      },
      "source": [
        "### 2.7 Stop Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YGHIAcUXvef",
        "outputId": "db7948e0-e759-4b5d-a4ed-5597eb10e077"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.4956 - accuracy: 0.8257\n",
            "Epoch 2/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3774 - accuracy: 0.8643\n",
            "Epoch 3/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3403 - accuracy: 0.8753\n",
            "Epoch 4/50\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.3136 - accuracy: 0.8861\n",
            "Epoch 5/50\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2946 - accuracy: 0.8921\n",
            "Epoch 6/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2808 - accuracy: 0.8969\n",
            "Epoch 7/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2689 - accuracy: 0.8997\n",
            "Epoch 8/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2586 - accuracy: 0.9035\n",
            "Epoch 9/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2477 - accuracy: 0.9072\n",
            "Epoch 10/50\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2418 - accuracy: 0.9096\n",
            "Epoch 11/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2316 - accuracy: 0.9140\n",
            "Epoch 12/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2248 - accuracy: 0.9156\n",
            "Epoch 13/50\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2182 - accuracy: 0.9184\n",
            "Epoch 14/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2119 - accuracy: 0.9202\n",
            "Epoch 15/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2074 - accuracy: 0.9218\n",
            "Epoch 16/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2008 - accuracy: 0.9244\n",
            "Epoch 17/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1942 - accuracy: 0.9271\n",
            "Epoch 18/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1907 - accuracy: 0.9290\n",
            "Epoch 19/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1844 - accuracy: 0.9300\n",
            "Epoch 20/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1809 - accuracy: 0.9324\n",
            "Epoch 21/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1760 - accuracy: 0.9343\n",
            "Epoch 22/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1725 - accuracy: 0.9353\n",
            "Epoch 23/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1683 - accuracy: 0.9355\n",
            "Epoch 24/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1646 - accuracy: 0.9392\n",
            "Epoch 25/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1600 - accuracy: 0.9401\n",
            "Epoch 26/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1573 - accuracy: 0.9409\n",
            "Epoch 27/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1536 - accuracy: 0.9431\n",
            "Epoch 28/50\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.1523 - accuracy: 0.9432\n",
            "Epoch 29/50\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.1469 - accuracy: 0.9436\n",
            "Epoch 30/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1429 - accuracy: 0.9458\n",
            "Epoch 31/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1402 - accuracy: 0.9474\n",
            "Epoch 32/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1365 - accuracy: 0.9483\n",
            "Epoch 33/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1358 - accuracy: 0.9495\n",
            "Epoch 34/50\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1337 - accuracy: 0.9495\n",
            "Epoch 35/50\n",
            "1863/1875 [============================>.] - ETA: 0s - loss: 0.1288 - accuracy: 0.9516\n",
            "Reached 95% accuracy so cancelling training!\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1289 - accuracy: 0.9515\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f313ff5c810>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    if logs.get('accuracy') > 0.95:\n",
        "      print(\"\\nReached 95% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "callbacks = myCallback()\n",
        "\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(training_images, training_labels), (test_images, test_labels) = data.load_data()\n",
        "\n",
        "training_images = training_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "            tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "            tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "        ])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=50,\n",
        "          callbacks=[callbacks])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9u4hYlqdY_Uj"
      },
      "source": [
        "## Chapter 3 Development from the Basics: Feature Detection in Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jI6vpTVpzsL"
      },
      "source": [
        "### 3.3 Convolutional Neural Network Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "eSESPnR3q4AN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "1c305156-d3ad-4da0-8024-9a85b224e4d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1875/1875 [==============================] - 90s 48ms/step - loss: 0.4326 - accuracy: 0.8428\n",
            "Epoch 2/50\n",
            "1253/1875 [===================>..........] - ETA: 30s - loss: 0.2968 - accuracy: 0.8915"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-c87935d493f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m               metrics=['accuracy'])\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "data = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(training_images, training_labels), (test_images, test_labels) = data.load_data()\n",
        "\n",
        "training_images = training_images.reshape(60000, 28, 28, 1)\n",
        "training_images = training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu',\n",
        "                           input_shape=(28, 28, 1)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=50)\n",
        "\n",
        "model.evaluate(test_images, test_labels)\n",
        "\n",
        "classifications = model.predict(test_images)\n",
        "print(classifications[0])\n",
        "print(test_labels[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgLzs_D0yu9i"
      },
      "source": [
        "### 3.4 A survey of convolutional networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAdKljgy0mji"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hl5lGzu0sc1"
      },
      "source": [
        "### 3.5 Building a CNN that discriminates between horses and humans"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7bgzqxs118k"
      },
      "source": [
        "#### Keras ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ED6WjgCX1_JJ"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "url = \"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\"\n",
        "file_name = \"horse-or-human.zip\"\n",
        "training_dir = 'horse-or-human/training/'\n",
        "urllib.request.urlretrieve(url, file_name)\n",
        "\n",
        "zip_ref = zipfile.ZipFile(file_name, 'r')\n",
        "zip_ref.extractall(training_dir)\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1mTpdkk4Mjh"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    training_dir,\n",
        "    target_size=(300, 300),\n",
        "    class_mode='binary'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK1y_cYg4dbV"
      },
      "source": [
        "#### CNN architecture for horse and human datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4BtC00p5OuB"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu',\n",
        "                           input_shape=(300, 300, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjuWZvtl9xam"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bOKKxv6-CFv"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(lr=0.001),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtrRaN2MDdi0"
      },
      "outputs": [],
      "source": [
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    epochs=15\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlfXJyqXDmMm"
      },
      "source": [
        "#### Validate the horse and human model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMQkVMxPFs41"
      },
      "outputs": [],
      "source": [
        "validation_url = \"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip\"\n",
        "\n",
        "validation_file_name = \"validation-horse-or-human.zip\"\n",
        "validation_dir = 'horse-or-human/validation/'\n",
        "urllib.request.urlretrieve(validation_url, validation_file_name)\n",
        "\n",
        "zip_ref = zipfile.ZipFile(validation_file_name, 'r')\n",
        "zip_ref.extractall(validation_dir)\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNMXZH2iKzAV"
      },
      "outputs": [],
      "source": [
        "validation_datagen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(300, 300),\n",
        "    class_mode='binary'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BG2q2SpmLlsE"
      },
      "outputs": [],
      "source": [
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    epochs=15,\n",
        "    validation_data=validation_generator\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucDqB4JmL2wY"
      },
      "source": [
        "#### Testing Horse and Human Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuBsY1PsMIuY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "\n",
        "  # predicting images\n",
        "  path = '/content/' + fn\n",
        "  img = image.load_img(path, target_size=(300, 300))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  image_tensor = np.vstack([x])\n",
        "  classes = model.predict(image_tensor)\n",
        "  print(classes)\n",
        "  print(classes[0])\n",
        "  if classes[0]>0.5:\n",
        "    print(fn + \"is a human\")\n",
        "  else:\n",
        "    print(fn + \"is a horse\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.6 Image Augmentation"
      ],
      "metadata": {
        "id": "a_TD6tskYVTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops.gen_nn_ops import batch_norm_with_global_normalization_grad\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1/255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest')\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    training_dir,\n",
        "    target_size=(300, 300),\n",
        "    batch_size=128,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(300, 300),\n",
        "    class_mode='binary'\n",
        ")"
      ],
      "metadata": {
        "id": "hQs8BYXMY0My"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=8,\n",
        "    epochs=15,\n",
        "    verbose=1,\n",
        "    validation_data=validation_generator\n",
        ")"
      ],
      "metadata": {
        "id": "l6G7AEth3SSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "\n",
        "  # predicting images\n",
        "  path = '/content/' + fn\n",
        "  img = image.load_img(path, target_size=(300, 300))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  image_tensor = np.vstack([x])\n",
        "  classes = model.predict(image_tensor)\n",
        "  print(classes)\n",
        "  print(classes[0])\n",
        "  if classes[0]>0.5:\n",
        "    print(fn + \"is a human\")\n",
        "  else:\n",
        "    print(fn + \"is a horse\")"
      ],
      "metadata": {
        "id": "nDilDSEq3szL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.7 Transfer Learning"
      ],
      "metadata": {
        "id": "c2kimPul7Hhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "import os \n",
        "import zipfile\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "weights_url = \"https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
        "\n",
        "weights_file = \"inception_v3.h5\"\n",
        "urllib.request.urlretrieve(weights_url, weights_file)\n",
        "\n",
        "pre_trained_model = InceptionV3(input_shape=(150, 150, 3),\n",
        "                                include_top=False,\n",
        "                                weights=None)\n",
        "\n",
        "pre_trained_model.load_weights(weights_file)"
      ],
      "metadata": {
        "id": "czypgcDp7SHK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_trained_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_O2gkbtDPfn",
        "outputId": "a049bd67-db48-44e8-e10b-1d7901e93335"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"inception_v3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 150, 150, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 74, 74, 32)   864         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 74, 74, 32)  96          ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 74, 74, 32)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 72, 72, 32)   9216        ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 72, 72, 32)  96          ['conv2d_1[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 72, 72, 32)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 72, 72, 64)   18432       ['activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 72, 72, 64)  192         ['conv2d_2[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 72, 72, 64)   0           ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2D)   (None, 35, 35, 64)   0           ['activation_2[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 35, 35, 80)   5120        ['max_pooling2d[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 35, 35, 80)  240         ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 35, 35, 80)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 33, 33, 192)  138240      ['activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 33, 33, 192)  576        ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 33, 33, 192)  0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 192)  0          ['activation_4[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 16, 16, 64)   12288       ['max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 16, 16, 64)  192         ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 16, 16, 64)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 16, 16, 48)   9216        ['max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 16, 16, 96)   55296       ['activation_8[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 16, 16, 48)  144         ['conv2d_6[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 16, 16, 96)  288         ['conv2d_9[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 16, 16, 48)   0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 16, 16, 96)   0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " average_pooling2d (AveragePool  (None, 16, 16, 192)  0          ['max_pooling2d_1[0][0]']        \n",
            " ing2D)                                                                                           \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 16, 16, 64)   12288       ['max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 16, 16, 64)   76800       ['activation_6[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 16, 16, 96)   82944       ['activation_9[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 16, 16, 32)   6144        ['average_pooling2d[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 16, 16, 64)  192         ['conv2d_5[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 16, 16, 64)  192         ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 16, 16, 96)  288         ['conv2d_10[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 16, 16, 32)  96          ['conv2d_11[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 16, 16, 64)   0           ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 16, 16, 64)   0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 16, 16, 96)   0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " mixed0 (Concatenate)           (None, 16, 16, 256)  0           ['activation_5[0][0]',           \n",
            "                                                                  'activation_7[0][0]',           \n",
            "                                                                  'activation_10[0][0]',          \n",
            "                                                                  'activation_11[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 16, 16, 64)   16384       ['mixed0[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 16, 16, 64)  192         ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 16, 16, 64)   0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 16, 16, 48)   12288       ['mixed0[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 16, 16, 96)   55296       ['activation_15[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 16, 16, 48)  144         ['conv2d_13[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 16, 16, 96)  288         ['conv2d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 16, 16, 48)   0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " activation_16 (Activation)     (None, 16, 16, 96)   0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " average_pooling2d_1 (AveragePo  (None, 16, 16, 256)  0          ['mixed0[0][0]']                 \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 16, 16, 64)   16384       ['mixed0[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 16, 16, 64)   76800       ['activation_13[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 16, 16, 96)   82944       ['activation_16[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 16, 16, 64)   16384       ['average_pooling2d_1[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 16, 16, 64)  192         ['conv2d_12[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 16, 16, 64)  192         ['conv2d_14[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 16, 16, 96)  288         ['conv2d_17[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 16, 16, 64)  192         ['conv2d_18[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 16, 16, 64)   0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 16, 16, 64)   0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " activation_17 (Activation)     (None, 16, 16, 96)   0           ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " activation_18 (Activation)     (None, 16, 16, 64)   0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " mixed1 (Concatenate)           (None, 16, 16, 288)  0           ['activation_12[0][0]',          \n",
            "                                                                  'activation_14[0][0]',          \n",
            "                                                                  'activation_17[0][0]',          \n",
            "                                                                  'activation_18[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 16, 16, 64)   18432       ['mixed1[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 16, 16, 64)  192         ['conv2d_22[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_22 (Activation)     (None, 16, 16, 64)   0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 16, 16, 48)   13824       ['mixed1[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 16, 16, 96)   55296       ['activation_22[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 16, 16, 48)  144         ['conv2d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 16, 16, 96)  288         ['conv2d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_20 (Activation)     (None, 16, 16, 48)   0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " activation_23 (Activation)     (None, 16, 16, 96)   0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " average_pooling2d_2 (AveragePo  (None, 16, 16, 288)  0          ['mixed1[0][0]']                 \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 16, 16, 64)   18432       ['mixed1[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 16, 16, 64)   76800       ['activation_20[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 16, 16, 96)   82944       ['activation_23[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 16, 16, 64)   18432       ['average_pooling2d_2[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 16, 16, 64)  192         ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 16, 16, 64)  192         ['conv2d_21[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 16, 16, 96)  288         ['conv2d_24[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 16, 16, 64)  192         ['conv2d_25[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_19 (Activation)     (None, 16, 16, 64)   0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " activation_21 (Activation)     (None, 16, 16, 64)   0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " activation_24 (Activation)     (None, 16, 16, 96)   0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " activation_25 (Activation)     (None, 16, 16, 64)   0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " mixed2 (Concatenate)           (None, 16, 16, 288)  0           ['activation_19[0][0]',          \n",
            "                                                                  'activation_21[0][0]',          \n",
            "                                                                  'activation_24[0][0]',          \n",
            "                                                                  'activation_25[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 16, 16, 64)   18432       ['mixed2[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 16, 16, 64)  192         ['conv2d_27[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_27 (Activation)     (None, 16, 16, 64)   0           ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 16, 16, 96)   55296       ['activation_27[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 16, 16, 96)  288         ['conv2d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_28 (Activation)     (None, 16, 16, 96)   0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 7, 7, 384)    995328      ['mixed2[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 7, 7, 96)     82944       ['activation_28[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 7, 7, 384)   1152        ['conv2d_26[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 7, 7, 96)    288         ['conv2d_29[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_26 (Activation)     (None, 7, 7, 384)    0           ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " activation_29 (Activation)     (None, 7, 7, 96)     0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPooling2D)  (None, 7, 7, 288)   0           ['mixed2[0][0]']                 \n",
            "                                                                                                  \n",
            " mixed3 (Concatenate)           (None, 7, 7, 768)    0           ['activation_26[0][0]',          \n",
            "                                                                  'activation_29[0][0]',          \n",
            "                                                                  'max_pooling2d_2[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 7, 7, 128)    98304       ['mixed3[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 7, 7, 128)   384         ['conv2d_34[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_34 (Activation)     (None, 7, 7, 128)    0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 7, 7, 128)    114688      ['activation_34[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 7, 7, 128)   384         ['conv2d_35[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_35 (Activation)     (None, 7, 7, 128)    0           ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 7, 7, 128)    98304       ['mixed3[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 7, 7, 128)    114688      ['activation_35[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 7, 7, 128)   384         ['conv2d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 7, 7, 128)   384         ['conv2d_36[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_31 (Activation)     (None, 7, 7, 128)    0           ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " activation_36 (Activation)     (None, 7, 7, 128)    0           ['batch_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 7, 7, 128)    114688      ['activation_31[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 7, 7, 128)    114688      ['activation_36[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 7, 7, 128)   384         ['conv2d_32[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 7, 7, 128)   384         ['conv2d_37[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_32 (Activation)     (None, 7, 7, 128)    0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " activation_37 (Activation)     (None, 7, 7, 128)    0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " average_pooling2d_3 (AveragePo  (None, 7, 7, 768)   0           ['mixed3[0][0]']                 \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 7, 7, 192)    147456      ['mixed3[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 7, 7, 192)    172032      ['activation_32[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 7, 7, 192)    172032      ['activation_37[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 7, 7, 192)    147456      ['average_pooling2d_3[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 7, 7, 192)   576         ['conv2d_30[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 7, 7, 192)   576         ['conv2d_33[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 7, 7, 192)   576         ['conv2d_38[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 7, 7, 192)   576         ['conv2d_39[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_30 (Activation)     (None, 7, 7, 192)    0           ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " activation_33 (Activation)     (None, 7, 7, 192)    0           ['batch_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " activation_38 (Activation)     (None, 7, 7, 192)    0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " activation_39 (Activation)     (None, 7, 7, 192)    0           ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " mixed4 (Concatenate)           (None, 7, 7, 768)    0           ['activation_30[0][0]',          \n",
            "                                                                  'activation_33[0][0]',          \n",
            "                                                                  'activation_38[0][0]',          \n",
            "                                                                  'activation_39[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 7, 7, 160)    122880      ['mixed4[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 7, 7, 160)   480         ['conv2d_44[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_44 (Activation)     (None, 7, 7, 160)    0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 7, 7, 160)    179200      ['activation_44[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 7, 7, 160)   480         ['conv2d_45[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_45 (Activation)     (None, 7, 7, 160)    0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 7, 7, 160)    122880      ['mixed4[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 7, 7, 160)    179200      ['activation_45[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 7, 7, 160)   480         ['conv2d_41[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 7, 7, 160)   480         ['conv2d_46[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_41 (Activation)     (None, 7, 7, 160)    0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " activation_46 (Activation)     (None, 7, 7, 160)    0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 7, 7, 160)    179200      ['activation_41[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 7, 7, 160)    179200      ['activation_46[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 7, 7, 160)   480         ['conv2d_42[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 7, 7, 160)   480         ['conv2d_47[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_42 (Activation)     (None, 7, 7, 160)    0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " activation_47 (Activation)     (None, 7, 7, 160)    0           ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " average_pooling2d_4 (AveragePo  (None, 7, 7, 768)   0           ['mixed4[0][0]']                 \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 7, 7, 192)    147456      ['mixed4[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 7, 7, 192)    215040      ['activation_42[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 7, 7, 192)    215040      ['activation_47[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 7, 7, 192)    147456      ['average_pooling2d_4[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 7, 7, 192)   576         ['conv2d_40[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 7, 7, 192)   576         ['conv2d_43[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 7, 7, 192)   576         ['conv2d_48[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_49 (BatchN  (None, 7, 7, 192)   576         ['conv2d_49[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_40 (Activation)     (None, 7, 7, 192)    0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " activation_43 (Activation)     (None, 7, 7, 192)    0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " activation_48 (Activation)     (None, 7, 7, 192)    0           ['batch_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            " activation_49 (Activation)     (None, 7, 7, 192)    0           ['batch_normalization_49[0][0]'] \n",
            "                                                                                                  \n",
            " mixed5 (Concatenate)           (None, 7, 7, 768)    0           ['activation_40[0][0]',          \n",
            "                                                                  'activation_43[0][0]',          \n",
            "                                                                  'activation_48[0][0]',          \n",
            "                                                                  'activation_49[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)             (None, 7, 7, 160)    122880      ['mixed5[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_54 (BatchN  (None, 7, 7, 160)   480         ['conv2d_54[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_54 (Activation)     (None, 7, 7, 160)    0           ['batch_normalization_54[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)             (None, 7, 7, 160)    179200      ['activation_54[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_55 (BatchN  (None, 7, 7, 160)   480         ['conv2d_55[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_55 (Activation)     (None, 7, 7, 160)    0           ['batch_normalization_55[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 7, 7, 160)    122880      ['mixed5[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)             (None, 7, 7, 160)    179200      ['activation_55[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_51 (BatchN  (None, 7, 7, 160)   480         ['conv2d_51[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_56 (BatchN  (None, 7, 7, 160)   480         ['conv2d_56[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_51 (Activation)     (None, 7, 7, 160)    0           ['batch_normalization_51[0][0]'] \n",
            "                                                                                                  \n",
            " activation_56 (Activation)     (None, 7, 7, 160)    0           ['batch_normalization_56[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 7, 7, 160)    179200      ['activation_51[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_57 (Conv2D)             (None, 7, 7, 160)    179200      ['activation_56[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_52 (BatchN  (None, 7, 7, 160)   480         ['conv2d_52[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_57 (BatchN  (None, 7, 7, 160)   480         ['conv2d_57[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_52 (Activation)     (None, 7, 7, 160)    0           ['batch_normalization_52[0][0]'] \n",
            "                                                                                                  \n",
            " activation_57 (Activation)     (None, 7, 7, 160)    0           ['batch_normalization_57[0][0]'] \n",
            "                                                                                                  \n",
            " average_pooling2d_5 (AveragePo  (None, 7, 7, 768)   0           ['mixed5[0][0]']                 \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 7, 7, 192)    147456      ['mixed5[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)             (None, 7, 7, 192)    215040      ['activation_52[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_58 (Conv2D)             (None, 7, 7, 192)    215040      ['activation_57[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_59 (Conv2D)             (None, 7, 7, 192)    147456      ['average_pooling2d_5[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_50 (BatchN  (None, 7, 7, 192)   576         ['conv2d_50[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_53 (BatchN  (None, 7, 7, 192)   576         ['conv2d_53[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_58 (BatchN  (None, 7, 7, 192)   576         ['conv2d_58[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_59 (BatchN  (None, 7, 7, 192)   576         ['conv2d_59[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_50 (Activation)     (None, 7, 7, 192)    0           ['batch_normalization_50[0][0]'] \n",
            "                                                                                                  \n",
            " activation_53 (Activation)     (None, 7, 7, 192)    0           ['batch_normalization_53[0][0]'] \n",
            "                                                                                                  \n",
            " activation_58 (Activation)     (None, 7, 7, 192)    0           ['batch_normalization_58[0][0]'] \n",
            "                                                                                                  \n",
            " activation_59 (Activation)     (None, 7, 7, 192)    0           ['batch_normalization_59[0][0]'] \n",
            "                                                                                                  \n",
            " mixed6 (Concatenate)           (None, 7, 7, 768)    0           ['activation_50[0][0]',          \n",
            "                                                                  'activation_53[0][0]',          \n",
            "                                                                  'activation_58[0][0]',          \n",
            "                                                                  'activation_59[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)             (None, 7, 7, 192)    147456      ['mixed6[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_64 (BatchN  (None, 7, 7, 192)   576         ['conv2d_64[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_64 (Activation)     (None, 7, 7, 192)    0           ['batch_normalization_64[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_65 (Conv2D)             (None, 7, 7, 192)    258048      ['activation_64[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_65 (BatchN  (None, 7, 7, 192)   576         ['conv2d_65[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_65 (Activation)     (None, 7, 7, 192)    0           ['batch_normalization_65[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_61 (Conv2D)             (None, 7, 7, 192)    147456      ['mixed6[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_66 (Conv2D)             (None, 7, 7, 192)    258048      ['activation_65[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_61 (BatchN  (None, 7, 7, 192)   576         ['conv2d_61[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_66 (BatchN  (None, 7, 7, 192)   576         ['conv2d_66[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_61 (Activation)     (None, 7, 7, 192)    0           ['batch_normalization_61[0][0]'] \n",
            "                                                                                                  \n",
            " activation_66 (Activation)     (None, 7, 7, 192)    0           ['batch_normalization_66[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_62 (Conv2D)             (None, 7, 7, 192)    258048      ['activation_61[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_67 (Conv2D)             (None, 7, 7, 192)    258048      ['activation_66[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_62 (BatchN  (None, 7, 7, 192)   576         ['conv2d_62[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_67 (BatchN  (None, 7, 7, 192)   576         ['conv2d_67[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_62 (Activation)     (None, 7, 7, 192)    0           ['batch_normalization_62[0][0]'] \n",
            "                                                                                                  \n",
            " activation_67 (Activation)     (None, 7, 7, 192)    0           ['batch_normalization_67[0][0]'] \n",
            "                                                                                                  \n",
            " average_pooling2d_6 (AveragePo  (None, 7, 7, 768)   0           ['mixed6[0][0]']                 \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2d_60 (Conv2D)             (None, 7, 7, 192)    147456      ['mixed6[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)             (None, 7, 7, 192)    258048      ['activation_62[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_68 (Conv2D)             (None, 7, 7, 192)    258048      ['activation_67[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_69 (Conv2D)             (None, 7, 7, 192)    147456      ['average_pooling2d_6[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_60 (BatchN  (None, 7, 7, 192)   576         ['conv2d_60[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_63 (BatchN  (None, 7, 7, 192)   576         ['conv2d_63[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_68 (BatchN  (None, 7, 7, 192)   576         ['conv2d_68[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_69 (BatchN  (None, 7, 7, 192)   576         ['conv2d_69[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_60 (Activation)     (None, 7, 7, 192)    0           ['batch_normalization_60[0][0]'] \n",
            "                                                                                                  \n",
            " activation_63 (Activation)     (None, 7, 7, 192)    0           ['batch_normalization_63[0][0]'] \n",
            "                                                                                                  \n",
            " activation_68 (Activation)     (None, 7, 7, 192)    0           ['batch_normalization_68[0][0]'] \n",
            "                                                                                                  \n",
            " activation_69 (Activation)     (None, 7, 7, 192)    0           ['batch_normalization_69[0][0]'] \n",
            "                                                                                                  \n",
            " mixed7 (Concatenate)           (None, 7, 7, 768)    0           ['activation_60[0][0]',          \n",
            "                                                                  'activation_63[0][0]',          \n",
            "                                                                  'activation_68[0][0]',          \n",
            "                                                                  'activation_69[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_72 (Conv2D)             (None, 7, 7, 192)    147456      ['mixed7[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_72 (BatchN  (None, 7, 7, 192)   576         ['conv2d_72[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_72 (Activation)     (None, 7, 7, 192)    0           ['batch_normalization_72[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_73 (Conv2D)             (None, 7, 7, 192)    258048      ['activation_72[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_73 (BatchN  (None, 7, 7, 192)   576         ['conv2d_73[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_73 (Activation)     (None, 7, 7, 192)    0           ['batch_normalization_73[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_70 (Conv2D)             (None, 7, 7, 192)    147456      ['mixed7[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_74 (Conv2D)             (None, 7, 7, 192)    258048      ['activation_73[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_70 (BatchN  (None, 7, 7, 192)   576         ['conv2d_70[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_74 (BatchN  (None, 7, 7, 192)   576         ['conv2d_74[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_70 (Activation)     (None, 7, 7, 192)    0           ['batch_normalization_70[0][0]'] \n",
            "                                                                                                  \n",
            " activation_74 (Activation)     (None, 7, 7, 192)    0           ['batch_normalization_74[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_71 (Conv2D)             (None, 3, 3, 320)    552960      ['activation_70[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_75 (Conv2D)             (None, 3, 3, 192)    331776      ['activation_74[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_71 (BatchN  (None, 3, 3, 320)   960         ['conv2d_71[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_75 (BatchN  (None, 3, 3, 192)   576         ['conv2d_75[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_71 (Activation)     (None, 3, 3, 320)    0           ['batch_normalization_71[0][0]'] \n",
            "                                                                                                  \n",
            " activation_75 (Activation)     (None, 3, 3, 192)    0           ['batch_normalization_75[0][0]'] \n",
            "                                                                                                  \n",
            " max_pooling2d_3 (MaxPooling2D)  (None, 3, 3, 768)   0           ['mixed7[0][0]']                 \n",
            "                                                                                                  \n",
            " mixed8 (Concatenate)           (None, 3, 3, 1280)   0           ['activation_71[0][0]',          \n",
            "                                                                  'activation_75[0][0]',          \n",
            "                                                                  'max_pooling2d_3[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_80 (Conv2D)             (None, 3, 3, 448)    573440      ['mixed8[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_80 (BatchN  (None, 3, 3, 448)   1344        ['conv2d_80[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_80 (Activation)     (None, 3, 3, 448)    0           ['batch_normalization_80[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_77 (Conv2D)             (None, 3, 3, 384)    491520      ['mixed8[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_81 (Conv2D)             (None, 3, 3, 384)    1548288     ['activation_80[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_77 (BatchN  (None, 3, 3, 384)   1152        ['conv2d_77[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_81 (BatchN  (None, 3, 3, 384)   1152        ['conv2d_81[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_77 (Activation)     (None, 3, 3, 384)    0           ['batch_normalization_77[0][0]'] \n",
            "                                                                                                  \n",
            " activation_81 (Activation)     (None, 3, 3, 384)    0           ['batch_normalization_81[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_78 (Conv2D)             (None, 3, 3, 384)    442368      ['activation_77[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_79 (Conv2D)             (None, 3, 3, 384)    442368      ['activation_77[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_82 (Conv2D)             (None, 3, 3, 384)    442368      ['activation_81[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_83 (Conv2D)             (None, 3, 3, 384)    442368      ['activation_81[0][0]']          \n",
            "                                                                                                  \n",
            " average_pooling2d_7 (AveragePo  (None, 3, 3, 1280)  0           ['mixed8[0][0]']                 \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2d_76 (Conv2D)             (None, 3, 3, 320)    409600      ['mixed8[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_78 (BatchN  (None, 3, 3, 384)   1152        ['conv2d_78[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_79 (BatchN  (None, 3, 3, 384)   1152        ['conv2d_79[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_82 (BatchN  (None, 3, 3, 384)   1152        ['conv2d_82[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_83 (BatchN  (None, 3, 3, 384)   1152        ['conv2d_83[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_84 (Conv2D)             (None, 3, 3, 192)    245760      ['average_pooling2d_7[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_76 (BatchN  (None, 3, 3, 320)   960         ['conv2d_76[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_78 (Activation)     (None, 3, 3, 384)    0           ['batch_normalization_78[0][0]'] \n",
            "                                                                                                  \n",
            " activation_79 (Activation)     (None, 3, 3, 384)    0           ['batch_normalization_79[0][0]'] \n",
            "                                                                                                  \n",
            " activation_82 (Activation)     (None, 3, 3, 384)    0           ['batch_normalization_82[0][0]'] \n",
            "                                                                                                  \n",
            " activation_83 (Activation)     (None, 3, 3, 384)    0           ['batch_normalization_83[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_84 (BatchN  (None, 3, 3, 192)   576         ['conv2d_84[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_76 (Activation)     (None, 3, 3, 320)    0           ['batch_normalization_76[0][0]'] \n",
            "                                                                                                  \n",
            " mixed9_0 (Concatenate)         (None, 3, 3, 768)    0           ['activation_78[0][0]',          \n",
            "                                                                  'activation_79[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 3, 3, 768)    0           ['activation_82[0][0]',          \n",
            "                                                                  'activation_83[0][0]']          \n",
            "                                                                                                  \n",
            " activation_84 (Activation)     (None, 3, 3, 192)    0           ['batch_normalization_84[0][0]'] \n",
            "                                                                                                  \n",
            " mixed9 (Concatenate)           (None, 3, 3, 2048)   0           ['activation_76[0][0]',          \n",
            "                                                                  'mixed9_0[0][0]',               \n",
            "                                                                  'concatenate[0][0]',            \n",
            "                                                                  'activation_84[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_89 (Conv2D)             (None, 3, 3, 448)    917504      ['mixed9[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_89 (BatchN  (None, 3, 3, 448)   1344        ['conv2d_89[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_89 (Activation)     (None, 3, 3, 448)    0           ['batch_normalization_89[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_86 (Conv2D)             (None, 3, 3, 384)    786432      ['mixed9[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_90 (Conv2D)             (None, 3, 3, 384)    1548288     ['activation_89[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_86 (BatchN  (None, 3, 3, 384)   1152        ['conv2d_86[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_90 (BatchN  (None, 3, 3, 384)   1152        ['conv2d_90[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_86 (Activation)     (None, 3, 3, 384)    0           ['batch_normalization_86[0][0]'] \n",
            "                                                                                                  \n",
            " activation_90 (Activation)     (None, 3, 3, 384)    0           ['batch_normalization_90[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_87 (Conv2D)             (None, 3, 3, 384)    442368      ['activation_86[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_88 (Conv2D)             (None, 3, 3, 384)    442368      ['activation_86[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_91 (Conv2D)             (None, 3, 3, 384)    442368      ['activation_90[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_92 (Conv2D)             (None, 3, 3, 384)    442368      ['activation_90[0][0]']          \n",
            "                                                                                                  \n",
            " average_pooling2d_8 (AveragePo  (None, 3, 3, 2048)  0           ['mixed9[0][0]']                 \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2d_85 (Conv2D)             (None, 3, 3, 320)    655360      ['mixed9[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_87 (BatchN  (None, 3, 3, 384)   1152        ['conv2d_87[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_88 (BatchN  (None, 3, 3, 384)   1152        ['conv2d_88[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_91 (BatchN  (None, 3, 3, 384)   1152        ['conv2d_91[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_92 (BatchN  (None, 3, 3, 384)   1152        ['conv2d_92[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_93 (Conv2D)             (None, 3, 3, 192)    393216      ['average_pooling2d_8[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_85 (BatchN  (None, 3, 3, 320)   960         ['conv2d_85[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_87 (Activation)     (None, 3, 3, 384)    0           ['batch_normalization_87[0][0]'] \n",
            "                                                                                                  \n",
            " activation_88 (Activation)     (None, 3, 3, 384)    0           ['batch_normalization_88[0][0]'] \n",
            "                                                                                                  \n",
            " activation_91 (Activation)     (None, 3, 3, 384)    0           ['batch_normalization_91[0][0]'] \n",
            "                                                                                                  \n",
            " activation_92 (Activation)     (None, 3, 3, 384)    0           ['batch_normalization_92[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_93 (BatchN  (None, 3, 3, 192)   576         ['conv2d_93[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_85 (Activation)     (None, 3, 3, 320)    0           ['batch_normalization_85[0][0]'] \n",
            "                                                                                                  \n",
            " mixed9_1 (Concatenate)         (None, 3, 3, 768)    0           ['activation_87[0][0]',          \n",
            "                                                                  'activation_88[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 3, 3, 768)    0           ['activation_91[0][0]',          \n",
            "                                                                  'activation_92[0][0]']          \n",
            "                                                                                                  \n",
            " activation_93 (Activation)     (None, 3, 3, 192)    0           ['batch_normalization_93[0][0]'] \n",
            "                                                                                                  \n",
            " mixed10 (Concatenate)          (None, 3, 3, 2048)   0           ['activation_85[0][0]',          \n",
            "                                                                  'mixed9_1[0][0]',               \n",
            "                                                                  'concatenate_1[0][0]',          \n",
            "                                                                  'activation_93[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 21,802,784\n",
            "Trainable params: 21,768,352\n",
            "Non-trainable params: 34,432\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in pre_trained_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "last_layer = pre_trained_model.get_layer('mixed7')\n",
        "print('last layer output shape: ', last_layer.output_shape)\n",
        "last_output = last_layer.output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBJy8zqWFDuV",
        "outputId": "824d20c1-9971-46fc-d440-f829250c8002"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "last layer output shape:  (None, 7, 7, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the output layer to 1 dimension\n",
        "x = layers.Flatten()(last_output)\n",
        "# Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
        "x = layers.Dense(1024, activation='relu')(x)\n",
        "# Add a dropout rate of 0.2\n",
        "x = layers.Dropout(0.2)(x)\n",
        "# Add a final sigmoid layer for classification\n",
        "x = layers.Dense(1, activation='sigmoid')(x)"
      ],
      "metadata": {
        "id": "2O3-WbGAG3Wh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(pre_trained_model.input, x)\n",
        "\n",
        "model.compile(optimizer=RMSprop(lr=0.0001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2tF-B5ZIC3h",
        "outputId": "6d4b0548-8e40-4a47-b326-3ad19a6574f5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_url = \"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\"\n",
        "training_file_name = \"horse-or-human.zip\"\n",
        "training_dir = 'horse-or-human/training/'\n",
        "urllib.request.urlretrieve(training_url, training_file_name)\n",
        "zip_ref = zipfile.ZipFile(training_file_name, 'r')\n",
        "zip_ref.extractall(training_dir)\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "LDadhzVpIZnQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_url = \"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip\"\n",
        "validation_file_name = \"validation-horse-or-human.zip\"\n",
        "validation_dir = 'horse-or-human/validation/'\n",
        "urllib.request.urlretrieve(validation_url, validation_file_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-HGpIA-Kj3i",
        "outputId": "0498a531-f101-4e1f-9df2-fb89f7434c28"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('validation-horse-or-human.zip', <http.client.HTTPMessage at 0x7f5f7b4bd490>)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_ref = zipfile.ZipFile(validation_file_name, 'r')\n",
        "zip_ref.extractall(validation_dir)\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "MsOHgJLzK1-V"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add our data-augmentation parameters to ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(rescale=1./255.,\n",
        "                                   rotation_range=40,\n",
        "                                   width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True)"
      ],
      "metadata": {
        "id": "mwSqPR_dK37R"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator(rescale=1.0/255.)"
      ],
      "metadata": {
        "id": "R56fc65NK6oG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flow training images in batches of 20 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(training_dir,\n",
        "                                                    batch_size=20,\n",
        "                                                    class_mode='binary',\n",
        "                                                    target_size=(150, 150))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ft8BMAL8K9U4",
        "outputId": "2dec2ea2-4d50-45ba-ecae-d88547c58bea"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1027 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Flow validation images in batches of 20 using test_datagen generator\n",
        "validation_generator =  test_datagen.flow_from_directory(validation_dir,\n",
        "                                                         batch_size=20,\n",
        "                                                         class_mode='binary',\n",
        "                                                         target_size=(150, 150))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PorGk3XkK_RY",
        "outputId": "f765ed0e-56e2-43b7-b4c2-e90a8ba06c86"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 256 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit_generator(\n",
        "            train_generator,\n",
        "            validation_data=validation_generator,\n",
        "            epochs=20,\n",
        "            verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qV0-GVx3LGJN",
        "outputId": "e231f35d-8765-40a2-bf05-c59060aaf624"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "52/52 [==============================] - 78s 1s/step - loss: 0.1511 - acc: 0.9426 - val_loss: 0.1156 - val_acc: 0.9531\n",
            "Epoch 2/20\n",
            "52/52 [==============================] - 74s 1s/step - loss: 0.0343 - acc: 0.9912 - val_loss: 0.0076 - val_acc: 0.9961\n",
            "Epoch 3/20\n",
            "52/52 [==============================] - 72s 1s/step - loss: 0.0203 - acc: 0.9922 - val_loss: 0.0476 - val_acc: 0.9844\n",
            "Epoch 4/20\n",
            "52/52 [==============================] - 73s 1s/step - loss: 0.0058 - acc: 0.9981 - val_loss: 0.1955 - val_acc: 0.9531\n",
            "Epoch 5/20\n",
            "52/52 [==============================] - 73s 1s/step - loss: 0.0236 - acc: 0.9942 - val_loss: 0.0563 - val_acc: 0.9844\n",
            "Epoch 6/20\n",
            "52/52 [==============================] - 71s 1s/step - loss: 0.0102 - acc: 0.9961 - val_loss: 0.0125 - val_acc: 0.9961\n",
            "Epoch 7/20\n",
            "52/52 [==============================] - 73s 1s/step - loss: 0.0152 - acc: 0.9951 - val_loss: 0.1296 - val_acc: 0.9648\n",
            "Epoch 8/20\n",
            "52/52 [==============================] - 71s 1s/step - loss: 0.0151 - acc: 0.9961 - val_loss: 0.0255 - val_acc: 0.9922\n",
            "Epoch 9/20\n",
            "52/52 [==============================] - 71s 1s/step - loss: 0.0058 - acc: 0.9961 - val_loss: 0.0208 - val_acc: 0.9922\n",
            "Epoch 10/20\n",
            "52/52 [==============================] - 73s 1s/step - loss: 0.0032 - acc: 0.9981 - val_loss: 0.0084 - val_acc: 0.9961\n",
            "Epoch 11/20\n",
            "52/52 [==============================] - 71s 1s/step - loss: 0.0026 - acc: 0.9981 - val_loss: 0.0347 - val_acc: 0.9922\n",
            "Epoch 12/20\n",
            "52/52 [==============================] - 72s 1s/step - loss: 0.0073 - acc: 0.9961 - val_loss: 0.0013 - val_acc: 1.0000\n",
            "Epoch 13/20\n",
            "52/52 [==============================] - 74s 1s/step - loss: 0.0088 - acc: 0.9981 - val_loss: 0.1488 - val_acc: 0.9766\n",
            "Epoch 14/20\n",
            "52/52 [==============================] - 71s 1s/step - loss: 0.0126 - acc: 0.9971 - val_loss: 0.1445 - val_acc: 0.9805\n",
            "Epoch 15/20\n",
            "52/52 [==============================] - 72s 1s/step - loss: 0.0024 - acc: 0.9990 - val_loss: 0.0612 - val_acc: 0.9883\n",
            "Epoch 16/20\n",
            "52/52 [==============================] - 72s 1s/step - loss: 0.0111 - acc: 0.9981 - val_loss: 0.1013 - val_acc: 0.9805\n",
            "Epoch 17/20\n",
            "52/52 [==============================] - 74s 1s/step - loss: 0.0083 - acc: 0.9981 - val_loss: 0.1089 - val_acc: 0.9883\n",
            "Epoch 18/20\n",
            "52/52 [==============================] - 73s 1s/step - loss: 0.0018 - acc: 0.9990 - val_loss: 0.1362 - val_acc: 0.9805\n",
            "Epoch 19/20\n",
            "52/52 [==============================] - 72s 1s/step - loss: 0.0041 - acc: 0.9971 - val_loss: 0.0844 - val_acc: 0.9883\n",
            "Epoch 20/20\n",
            "52/52 [==============================] - 74s 1s/step - loss: 0.0013 - acc: 0.9990 - val_loss: 0.1508 - val_acc: 0.9727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path = '/content/' + fn\n",
        "  img = image.load_img(path, target_size=(150, 150))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  image_tensor = np.vstack([x])\n",
        "  classes = model.predict(image_tensor)\n",
        "  print(classes)\n",
        "  print(classes[0])\n",
        "  if classes[0]>0.5:\n",
        "    print(fn + \" is a human\")\n",
        "  else:\n",
        "    print(fn + \" is a horse\")"
      ],
      "metadata": {
        "id": "TSYER4LfLGpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.8 Multi-class classification"
      ],
      "metadata": {
        "id": "JxkKiXb9LLhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KjBv5aewLn_r"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "AI and Machine Learning for Coders.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNKAoyEg51Mr+ntELlPAbpq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}